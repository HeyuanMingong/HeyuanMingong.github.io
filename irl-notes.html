<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #07889b;
      text-decoration: none;
    }
    
    a:focus, a:hover {
      color: #e37222;
      text-decoration: none;
    }
    
    body, td, th, tr, p, a {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 16px
    }
		
		p2 {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
    }
		
    strong {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 16px;
    }
    
    heading {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 24px;
			color: #e37222;
    }
    
		heading2 {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; 
    font-size: 20px;
    }
		
    papertitle {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 16px;
      font-weight: 700
    }
    
    name {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 40px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
	
	<style> 
		p{line-height:22px} 
		.divcss5-b p{margin:30px auto} 
	</style> 
	
  <link rel="icon" type="image/png" href="data/nju_logo_circle.png">
  <title>Zhi WANG</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
	
	<script type="text/x-mathjax-config">
  	MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	</script>
	<script type="text/javascript"
		src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
</head>

<body>
	<script type="text/x-mathjax-config">
  	MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	</script>
	<script type="text/javascript"
		src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
	
  <table width="61.8%" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p align="center">
                <strong><heading>Notes for the paper</heading></strong>
                <p>
                  <a href="index#irl"><papertitle>Incremental reinforcement learning with prioritized sweeping for dynamic environments</papertitle></a>,
									<br>
                                    <a href="index.html"><strong>Zhi Wang</strong></a>, Chunlin Chen, Han-Xiong Li, Daoyi Dong, and Tzyh-Jong Tarn, 
									<br>
									<em>IEEE/ASME Transactions on Mechatronics</em>, 2019. 
									<br><br>
									Click <a href="irl-chinese-notes.html">here</a> to get a Chinese version of paper notes.
                </p>
              </p>
            </td>
          </tr>
        </table>
				
				<hr>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Dynamic environments</heading>
              <p> Traditional RL algorithms are performed in stationary environments. However, in many real-world applications, the environments are often dynamic, where the agent's states, available actions, state transition functions, and corresponding rewards may change over time. </p>
            </td>
          </tr>
				</table>
					
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td width="100%" valign="middle">
              <heading>A simple example</heading>
            </td>
					</tr>
				</table>
					
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
            <td width="40%"><img src='data/IRL/IRLcon.jpg' width="300"></td>
            <td width="60%" valign="top">
              <p>
              	The agent has learned an optimal policy $\pi^*$ at a time period $t$. Then, the reward function of the MDP changes at a new time period $t'$. The agent should adjust its behavior to find a new optimal policy $\pi_n^*$ to adapt to the new environment.
								<br><br>
								<strong>The goal of incremental learning:</strong> fusing the new information into the existing knowledge system, adjusting the previously learned policy to a new one whenever the environment changes.
								<br><br>
								<strong>Problem statement:</strong> In particular, we consider dynamic environments where the reward functions may change over time.
              </p>
            </td>
					</tr>
        </table>
	
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td width="100%" valign="middle">
              <heading>Incremental learning vs. Transfer learning</heading>
							<p><strong>Transfer learning</strong>: usually requires a relatively large set of source tasks to obtain a good knowledge base for a target task.</p>
							<p><strong>Incremental learning</strong>: copes with learning tasks where training samples become available over time or the learning environment is ever-changing.
							</p>
            </td>
					</tr>
				</table>
	
				<hr>
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td width="100%" valign="middle">
              <strong><heading>The proposed method: Incremental Reinforcement Learning (IRL)</heading></strong>
            </td>
					</tr>
				</table>
	
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td width="100%" valign="middle">
              <heading><em>Step 1: Detection of drift environment</em></heading>
							<p>
								<strong>Drift environment</strong> generates new data and provides new information for the learner. As RL learns through interactions with the environment, i.e. using the reward $r$ received when the action $a$ is taken in the state $s$, environment drift in RL implies that the corresponding reward $r$ of a certain state-action pair $(s, a)$ has changed over time. 
								<br><br>
								<strong>The drift environment is defined as the set of all state-action pairs whose rewards in the new environment differ from those in the original one.</strong>
								<br><br>
								To obtain the drift environment, we need to observe the rewards in both the original environment (known) and the new environment (unknown) to obtain the changed part.
								Hence, we generate a <strong>detector-agent</strong> to randomly explore the new environment by executing a virtual RL process. For a given state-action pair in the new environment, the detector-agent needs to traverse it only once. Compared with a standard RL process, the computational complexity of drift detection may be neglected.
							</p>
            </td>
					</tr>
					
					<tr>
						<td width="100%" valign="middle">
              <heading><em>Step 2: Prioritized sweeping of drift environment</em></heading>
							<p>
								<strong>Basic setting:</strong> Though we cannot construct the complete model of the new environment temporarily, we can use the predictive model of the original environment to concurrently train the new policy.
								<br><br>
								However, due to the environment drift, the agent tends to execute the previous optimal policy and may be trapped, which will make it hard to find the real optimal policy. In other words, the new information (drift environment) tends to be in conflict with the existing knowledge (previous optimal policy). 
								<br><br>
								<strong>A new mechanism is necessary to weaken the conflict in advance.</strong>
								<br><br>
								Since the drift environment is the source of new information, we can update the state-action space of drift environment (and its neighbor environment) by dynamic programming with top priority, using the value functions from the original environment and the rewards from the new environment.
								<br><br>
								<strong>In this way, IRL fuses the new information (new reward function) into the existing knowledge system (previous optimal value functions), as well as weakening the conflict between them.</strong>
							</p>
            </td>
					</tr>
					
					<tr>
						<td width="100%" valign="middle">
              <heading><em>Step 3: Restart a learning process</em></heading>
							<p>
								With the detection and prioritized sweeping techniques for drift environment, the agent restarts a standard learning process and computes the final new optimal policy in an incremental way.
							</p>
            </td>
					</tr>
				</table>
	
	      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>The integrated algorithm</heading>
              <p> Env. stands for environment </p>
            </td>
          </tr>
					<tr>
            <td width="100%" align="center">
							<img src='data/IRL/IRLfc.png' width="500"></td>
					</tr>
				</table>
	
				<hr>
		    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Experiments</heading>
              <p>A simple maze case, a benchmark maze case, and an intelligent warehouse.</p>
            </td>
					</tr>
					
					<tr>
						<td width="100%" valign="middle">
							<heading>The simple maze</heading>
						</td>
					</tr>
				</table>
	
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
            <td width="50%">
							<p align="center">Original environment</p>
							<div align="center"><img src='data/IRL/sMaze.png' width="300"></div>
						</td>
            <td width="50%" valign="top">
							<p align="center">New environment</p>
							<div align="center"><img src='data/IRL/sMazeN.png' width="300"></div>
            </td>
					</tr>
				</table>
					
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td width="100%" valign="middle">
							<heading>The benchmark maze</heading>
						</td>
					</tr>
					
					<tr>
            <td width="100%" align="center">
							<img src='data/IRL/cMaze.png' width="500">
						</td>
					</tr>
				</table>
					
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td width="100%" valign="middle">
							<heading>An intelligent warehouse</heading>
						</td>
					</tr>
        </table>
	
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
            <td width="40%"><img src='data/IRL/kiva.png' width="300"></td>
            <td width="60%" valign="top">
              <p>
              	<strong>The large-scale Kiva robots system has been a promising application for commercial automatic transportation in Amazon warehouses in recent years.</strong>
								<br><br>
								Each storage shelf consists of several inventory pods and each pod consists of several resources. By order, a robot lifts and carries a pod at a time along with a preplanned path from the starting point to the goal, delivers it to the specific service stations which are appointed in the order and finally returns the pod back. 
								<br><br>
								<strong>In real applications, the storage shelves and the picking stations may change over time. Moreover, in multi-agent reinforcement learning (MARL) problems, the environment of a robot may change due to the movements of other robots.</strong>
              </p>
            </td>
					</tr>
        </table>
	
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
            <td width="100%">
							<p align="center">Original environment</p>
							<div align="center"><img src='data/IRL/wh.png' width="600"></div>
						</td>
					</tr>
					<tr>
            <td width="100%" valign="top">
							<p align="center">New environment</p>
							<div align="center"><img src='data/IRL/whN.png' width="600"></div>
            </td>
					</tr>
				</table>
	




      </td>
    </tr>
  </table>
</body>

</html>
