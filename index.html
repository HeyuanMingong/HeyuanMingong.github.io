<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #07889b;
      text-decoration: none;
    }
    
    a:focus, a:hover {
      color: #e37222;
      text-decoration: none;
    }
    
    body, td, th, tr, p, a {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 16px
    }
		
		p2 {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
    }
		
    strong {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 16px;
    }
    
    heading {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 24px;
			color: #e37222;
    }
    
		heading2 {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; 
    font-size: 20px;
    }
		
    papertitle {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 16px;
      font-weight: 700
    }
    
    name {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 40px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
	
	<style> 
		p{line-height:22px} 
		.divcss5-b p{margin:30px auto} 
	</style> 
	
  <link rel="icon" type="image/png" href="data/nju_logo_circle.png">
  <title>Zhi WANG</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
	
  <style type="text/css">
  body,td,th {
	font-family: Lato, Verdana, Helvetica, sans-serif;
}
  </style>
  <script type="text/x-mathjax-config">
  	MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	</script>
	<script type="text/javascript"
		src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
</head>

<body>
	<script type="text/x-mathjax-config">
  	MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	</script>
	<script type="text/javascript"
		src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
	
  <table width="880" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="80%" valign="middle">
              <p align="center">
                <name>Zhi WANG (王志)</name> 
								<p align="center"><heading2> Reinforcement Learning | Robotics </heading2></p>
							</p>
							<p>
								I am currently an Assistant Professor at the <a href="http://sme.nju.edu.cn/2031/list.htm">Department of Control and Systems Engineering</a>, Nanjing University, China, and has the visiting position at the <a href="https://www.unsw.adfa.edu.au/school-of-engineering-and-information-technology/">School of Engineering and Information Technology</a>, University of New South Wales, Canberra, Australia.
								Previously, I received the Ph.D. degree at City University of Hong Kong, China, in 2019, and the B.E. degree at Nanjing University, in 2015. 
								<br><br>
								
								I'm interested in reinforcement learning (RL), machine learning, and robotics. Specifically, I work on how learning algorithms can scale RL agents to dynamic environments, allowing them to autonomously adapt to the non-stationary task distributions in real-world domains. This includes a wide range of topics such as incremental learning, online learning, continual learning, lifelong learning, transfer learning, model-based learning, and meta-learning. <br><br>
								
								<strong>Prospective students</strong>, feel free to contact me by email if you are interested in pursuing a master or Ph.D. degree in artificial intelligence / machine learning / robotics at the Department of Control and Systems Engineering, Nanjing University. (有意向在南京大学控制与系统工程系攻读人工智能/机器学习/机器人学方向的硕士/博士学位的学生, 欢迎联系!)
							</p>

              <p align=center>
                <a href="mailto:njuwangzhi@gmail.com">njuwangzhi@gmail.com</a> &nbsp/&nbsp
                <a href="data/CV/cv_zhiwang.pdf">CV</a> &nbsp/&nbsp
                <a href="data/CV/zwang-bio.txt">Biography</a> &nbsp/&nbsp
                <a href="https://scholar.google.com.hk/citations?hl=en&user=cRXlxYcAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.github.com/HeyuanMingong">Github</a>
              </p>
            </td>
            <td width="20%">
              <img src="data/side.jpg" width="200">
            </td>
          </tr>
        </table>
			
				<table width="100%" align="center" border="0" cellpadding="20">
					<tr>
            <td width="100%" valign="middle">
               <strong><heading>Teaching</heading></strong>
							<p>
                <a href="rl_lecture/index.html"><papertitle>Reinforcement Learning</papertitle></a>, 
								Fall 2019 - Co-instructor
              </p>
						</td>
					</tr>
				</table>
	
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
							<strong><heading>Publications</heading></strong><br><br>
              <heading2> <i>Journal Articles</i> </heading2>
							
              <p id="irlcs">
                <a href="https://ieeexplore.ieee.org/document/8786875">
                  <papertitle>Incremental reinforcement learning in continuous spaces via policy relaxation and importance weighting</papertitle>,
                </a>
                <br>
                <strong>Zhi Wang</strong>,
                Han-Xiong Li,
								and <a href="http://sme.nju.edu.cn/ccl/list.htm">Chunlin Chen</a>,
                <br>
                <em>IEEE Transactions on Neural Networks and Learning Systems</em>, 2019.
                <br>
								<a href="data/IRL_CS/irlcs_manuscript.pdf">pdf</a> /
								<a href="data/IRL_CS/irlcs_bib.txt">BibTex</a> / 
                <a href="http://www.github.com/HeyuanMingong/irl_cs">code</a> 
                <br><p2>
								In "<em>Incremental reinforcement learning with prioritized sweeping for dynamic environments</em>", an incremental learning algorithm was first proposed for RL in dynamic environments, which only worked in RL problems with a discrete state-action space owing to involving a tabular form of comparing reward functions and the prioritized sweeping process. In this paper, we design a feasible incremental learning method that incorporates with the functiuon approximation framework, thus being capable of working in continuous state/action spaces.</p2>
							</p>
							

              <p id="irl">
                <a href="https://ieeexplore.ieee.org/document/8642342">
                  <papertitle>Incremental reinforcement learning with prioritized sweeping for dynamic environments</papertitle>,
                </a>
                <br>
                <strong>Zhi Wang</strong>,
                <a href="http://sme.nju.edu.cn/ccl/list.htm">Chunlin Chen</a>,
                Han-Xiong Li,
                <a href="https://research.unsw.edu.au/people/associate-professor-daoyi-dong">Daoyi Dong</a>, and Tzyh-Jong Tarn,
                <br>
                <em>IEEE/ASME Transactions on Mechatronics</em>, 2019.
                <br>
								<a href="data/IRL/IRL-Early-Access.pdf">pdf</a> / 
								<a href="data/IRL/IRL_bib.txt">BibTex</a> /
								<a href="http://www.github.com/HeyuanMingong/irl.git">code</a> 
                <br><p2>
                Traditional RL algorithms focus on learning in a stationary environment. We propose a novel Incremental Reinforcement Learning (IRL) algorithm for learning in dynamic environments where the reward function may change over time. IRL provides an appealing option for saving a significant amount of computational resources, while the dynamic environment scenario is supposed to hold in many challenging real-world domains.</p2>
							</p>

              <p>
                <a href="https://ieeexplore.ieee.org/document/8668561">
                  <papertitle>Reinforcement learning based optimal sensor placement for spatiotemporal modeling</papertitle></a>,
                <br>
                <strong>Zhi Wang</strong>, 
                Han-Xiong Li, and
                <a href="http://sme.nju.edu.cn/ccl/list.htm">Chunlin Chen</a>,
								</br>
                <em>IEEE Transactions on Cybernetics</em>, 2019.
                <br>
                <a href="data/OSP/OSP-Manuscript.pdf">pdf</a> / 
                <a href="data/OSP/OSP_bib.txt">BibTex</a>
              </p>

							<p>
                <a href="https://ieeexplore.ieee.org/document/8730299">
                  <papertitle>Dissimilarity analysis based multimode modeling for complex distributed parameter systems</papertitle>,
                </a>
                <br>
                <strong>Zhi Wang</strong>, and Han-Xiong Li,
                <br>
                <em>IEEE Transactions on Systems, Man, and Cybernetics: Systems</em>, 2019.
                <br>
                <a href="data/MMM/Multimode_Modeling.pdf">pdf</a> / 
                <a href="data/MMM/MMM_bib.txt">BibTeX</a>
              </p>

							<p>
                <a href="https://ieeexplore.ieee.org/abstract/document/8319440">
                  <papertitle>Incremental learning for online modeling of distributed parameter systems</papertitle>,
                </a>
                <br>
                <strong>Zhi Wang</strong>, and Han-Xiong Li,
                <br>
                <em>IEEE Transactions on Systems, Man, and Cybernetics: Systems</em>, 2018.
                <br>
                <a href="data/IKL/IKL_Proof.pdf">pdf</a> / 
                <a href="data/IKL/IKL_bib.txt">BibTeX</a>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
                <heading2> <i>Conference Papers</i> </heading2>

              <p>
                <a href="https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4709">
                  <papertitle>Better fine-tuning via instance weighting for text classification</papertitle>,
                </a>
                <br>
                <strong>Zhi Wang</strong>, Wei Bi, Yan Wang, and Xiaojiang Liu,
                <br>
                in: <em>Proceedings of the AAAI Conference on Artificial Intelligence </em>, 2019.
                <br>
                <a href="data/IW_Fit/IW-Fit-Published-Version.pdf">pdf</a> / 
                <a href="data/IW_Fit/iwfit_bib.txt">BibTeX</a> /
								<a href="data/IW_Fit/IW-Fit-Appendix.pdf">supplementary materials</a>
                <br><p2>
               	Previous fine-tuning works mainly focus on the pre-training stage and investigate how to pretrain a set of parameters that can help the target task most. In this paper, we propose an Instance Weighting based Finetuning (IW-Fit) method, which revises the fine-tuning stage to improve the final performance on the target domain. IW-Fit adjusts instance weights at each fine-tuning epoch dynamically. The designed instance weighting metrics used in IW-Fit are model-agnostic, which are easy to implement for general DNN-based classifiers.</p2>
              </p>
					
							<p>
                <a href="https://www.researchgate.net/publication/333385705_Incremental_Learning_Based_Subspace_Modeling_for_Distributed_Parameter_Systems">
                  <papertitle>Incremental learning based subspace modeling for distributed parameter systems</papertitle>,
                </a>
                <br>
                <strong>Zhi Wang</strong>, and Han-Xiong Li,
                <br>
                in: <em>Proceedings of the International Joint Conference on Neural Networks (IJCNN)</em>, 2019.
                <br>
                <a href="data/MMM/ism-final-submission.pdf">pdf</a> / 
                <a href="data/MMM/ism_bib.txt">BibTeX</a>
              </p>

							<p>
                <a href="https://ieeexplore.ieee.org/abstract/document/7578530">
                  <papertitle>A novel incremental learning scheme for reinforcement learning in dynamic environments</papertitle>,
                </a>
                <br>
                <strong>Zhi Wang</strong>,
                <a href="http://sme.nju.edu.cn/ccl/list.htm">Chunlin Chen</a>,
                Han-Xiong Li,
                <a href="https://research.unsw.edu.au/people/associate-professor-daoyi-dong">Daoyi Dong</a>, and Tzyh-Jong Tarn,
                <br>
                in: <em>Proceedings of the World Congress on Intelligent Control and Automation (WCICA)</em>, 2016.
                <br>
                <a href="data/IRL/IRL_WCICA_Proof.pdf">pdf</a> /
                <a href="data/IRL/IRL_WCICA_bib.txt">BibTeX</a>
              </p>
            </td>
          </tr>
        </table>

				<table width="100%" align="center" border="0" cellpadding="20">
					<tr>
            <td width="100%" valign="middle">
               <strong><heading>Invited Talks</heading></strong>
							<p>
                <a href="data/unsw_talk/irl_unsw.pdf">
                  <papertitle>Incremental reinforcement learning for dynamic environments</papertitle>,
                </a>
                <br>
                School of Engineering and Information Technology, University of New South Wales, Canberra, Apr. 2019.
              </p>
							
							<p>
                <a href="data/nju_talk/NJU_Report_DPS.pdf">
                  <papertitle>Learning based intelligent modeling for distributed parameter systems</papertitle>,
                </a>
                <br>
                Department of Control and Systems Engineering, Nanjing University, Oct. 2018.
              </p>
						</td>
					</tr>
				</table>

				<table width="100%" align="center" border="0" cellpadding="20">
					<tr>
            <td width="100%" valign="middle">
              <strong><heading>Service</heading></strong>
							<br><br>
							<heading2> <i>Journal Peer Review</i> </heading2>
							<p>
								IEEE Transactions on Neural Networks and Learning Systems <br>
								IEEE Transactions on Cyberbetics <br>
								IEEE Transactions on Systems, Man, and Cybernetics: Systems
							</p>
						</td>
					</tr>
				</table>

				<footer class="footer">
					<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
						<tr>
							<td width=35%>
								<br>
								<p align="left">
									<font size="2">
										Updated November 2019.
									</font>
								</p>
							</td>
							<td width=30% align="center">
								<a href="https://clustrmaps.com/site/1ao9d"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=7_K6mNOCiPZl5lOYVEFnCB96B-2XYTGT5AkYL_tZ8Jw&cl=ffffff" /></a>
							</td>
							<td width=35%>
								<br>
								<p align="right">
									<font size="2">
										<a href="https://jonbarron.info/">This guy makes a nice webpage.</a>
									</font>
								</p>
							</td>
						</tr>
					</table>  
				</footer>
      </td>
    </tr>
  </table>
</body>

</html>
