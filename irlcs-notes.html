<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #07889b;
      text-decoration: none;
    }
    
    a:focus, a:hover {
      color: #e37222;
      text-decoration: none;
    }
    
    body, td, th, tr, p, a {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 16px
    }
		
		p2 {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
    }
		
    strong {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 16px;
    }
    
    heading {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 24px;
			color: #e37222;
    }
    
		heading2 {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; 
    font-size: 20px;
    }
		
    papertitle {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 16px;
      font-weight: 700
    }
    
    name {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 40px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
	
	<style> 
		p{line-height:22px} 
		.divcss5-b p{margin:30px auto} 
	</style> 
	
  <link rel="icon" type="image/png" href="data/nju_logo_circle.png">
  <title>Zhi WANG</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
	
	<script type="text/x-mathjax-config">
  	MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	</script>
	<script type="text/javascript"
		src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
</head>

<body>
	<script type="text/x-mathjax-config">
  	MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	</script>
	<script type="text/javascript"
		src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
	
  <table width="61.8%" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p align="center">
                <strong><heading>Notes for the paper</heading></strong>
                <p>
                  <a href="index#irlcs"><papertitle>Incremental reinforcement learning in continuous spaces via policy relaxation and importance weighting</papertitle></a>,
									<br>
                    <a href="index.html"><strong>Zhi Wang</strong></a>, Han-Xiong Li, and Chunlin Chen, 
									<br>
									<em>IEEE Transactions on Neural Networks and Learning Systems</em>, 2019. 
                </p>
              </p>
            </td>
          </tr>
        </table>
				
				<hr>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Dynamic environments</heading>
              <p> Traditional RL algorithms are performed in stationary environments. However, in many real-world applications, the environments are often dynamic, where the agent's states, available actions, state transition functions, and corresponding rewards may change over time. </p>
            </td>
          </tr>
				</table>
					
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td width="100%" valign="middle">
              <heading>Transfer learning vs. Multi-task learning vs. Incremental learning</heading>
            </td>
					</tr>
				</table>
					
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
            <td width="50%"><img src='data/IRL_CS/transfer.png' width="400"></td>
            <td width="50%" valign="top">
              <p>
								<strong>Transfer RL</strong><br>
              	addresses the cross-task generalization problem, which reuses the knowledge from a set of related source domains to help the learning task in the target domain. <br><br>
								Different types of knowledge can be transferred, such as sample instances, policies, and value functions. With the advance in DRL, the researchers focus more on how to transfer the knowledge with the use of DNNs.
              </p>
            </td>
					</tr>
					<tr>
            <td width="50%"><img src='data/IRL_CS/multitask.png' width="400"></td>
            <td width="50%" valign="top">
              <p><br>
								<strong>Multi-task RL</strong><br>
              	aims to solve a fixed set of tasks simultaneously based on the assumption that the tasks share some similarities in components such as the reward structure, the transition dynamics, or the value function.
              </p>
            </td>
					</tr>
					<tr>
            <td width="50%"><img src='data/IRL_CS/incremental.png' width="400"></td>
            <td width="50%" valign="top">
              <p><br>
								<strong>Incremental RL</strong><br>
              	incrementally adjusts the previously learned policy to a new one that fits in the new environment whenever the environment changes, which offers an appealing alternative for fast adaptation to dynamic environments. <br><br>
								Only the learned function approximation is needed for the new environment, circumventing the necessity for repeatedly accessing or processing a potentially large set of source tasks.
              </p>
            </td>
					</tr>
        </table>
	
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td width="100%" valign="middle">
              <heading>Previous work</heading>
							<p>
								<a href="index#irl">Incremental reinforcement learning with prioritized sweeping for dynamic environments</a>, <br>
								involved a tabular form of comparing the reward functions and the prioritized sweeping process, and hence could only be applied for RL problems with a discrete state-action space.
								<br><br>
								We aim at designing a feasible incremental learning method that can <strong>incorporate with the function approximation framework in continuous spaces</strong>.
							</p>
            </td>
					</tr>
				</table>
	
				<hr>
	
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td width="100%" valign="middle">
              <heading>Incremental reinforcement learning in continuous spaces</heading>
							<p>
								When the environment changes, one can directly initialize the parameters of a function approximation from the previous optima that have learned some of feature representations (e.g., nodes in a neural network) of the state-action space in the original environment.
							</p>
            </td>
					</tr>
					
					<tr>
						<td width="100%" valign="middle">
              <heading2><em>Step 1: Policy relaxation</em></heading2>
							<p>
								When the environment changes, the agent tends to visit a small part of the whole state-action space when executing the previously learned policy, thus probably leading to a local optimum due to <strong>insufficient exploration</strong>. 
								Hence, we propose a policy relaxation mechanism to <strong>encourage proper exploration for better adaptation in the long term</strong>.
								<br><br>
								In the $k$ burn-in learning episodes, the agent is forced to execute a relaxed policy where actions are randomly selected from the available set. Let $\pi_{\theta}$ denote the learned policy with parameters $\theta$. Regarding the number of learning episodes $\eta$, the agent's behavior policy $\pi_r$ is relaxed as:
							</p>
							<p align="center">
								$\pi_r(a|s)=uniform(A(s))~~if~~\eta\le k~~else~~\pi_{\theta}(a|s)$
							</p>
							<p>
								However, in the k burn-in episodes, we would encounter the special difficulty due to a mismatch of distributions. We would like samples drawn from the distribution of the estimated policy $\pi_{\theta}$ but all we have are samples drawn from the distribution of another behavior policy $\pi_r$.
								We adopt the classical Monte-Carlo technique, <strong>importance sampling</strong>, to handle this kind of mismatch as:
							</p>
							<p align="center">
								$E_{\tau\sim\pi_{\theta}(\tau)}[r(\tau)]=\int_{\tau}\pi_{\theta}(\tau)r(\tau)d\tau=\int_{\tau}\frac{\pi_{\theta}(\tau)}{\pi_r(\tau)}r(\tau)\pi_r(\tau)=E_{\tau\sim\pi_r(\tau)}\left[\frac{\pi_{\theta}(\tau)}{\pi_r(\tau)}r(\tau)\right]$
							</p>
            </td>
					</tr>
					
					<tr>
						<td width="100%" valign="middle">
              <heading2><em>Step 2: Importance Weighting</em></heading2>
							<p>
								We denote three kinds of learning episodes as: <br>
								<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
									<tr>
										<td width="20%" align="right">$\tau_{t-1}$:</td>
										<td width="80%" align="left">generated by the learned optimal policy in the original environment</td>
									</tr>
									<tr>
										<td width="20%" align="right">$\tau_{r}$:</td>
										<td width="80%" align="left">generated by a randomly-initialized policy</td>
									</tr>
									<tr>
										<td width="20%" align="right">$\tau_t^1,\tau_t^2$:</td>
										<td width="80%" align="left">generated by the current policy that is initialized from the original environment</td>
									</tr>
								</table>
							</p>
            </td>
					</tr>
					
					<tr>
						<td width="40%" align="center">
							<img src='data/IRL_CS/iw.png' width="500">
						</td>
					</tr>
	
					<tr>
						<td width="100%" valign="middle">
							<p>
								<strong>Empirical benifit</strong>: $r(\tau_t^1), r(\tau_t^2) > r(\tau_r)$ <br>
								Episodes generated by the current policy tend to receive higher returns than those generated by a random policy. Initializing parameters from the original environment empirically benefits the learning process when starting to interact with the new environment.
							</p>
							<p>
								<strong>Motivation</strong> <br>
								We need to <strong>encourage the policies to move towards promising regions of parameter space</strong> that better fits in the new environment, which may be far away from the previous optimum.
							</p>
							<p>
								<strong>Insight</strong> <br>
								Due to the environment change, the two example episodes, $\tau_t^1$ and $\tau_t^2$, cannot obtain satisfactory learning performance yet in the new environment. Nevertheless, compared to $\tau_t^1$, the episode $\tau_t^2$ is closer to the new optimal path, and receives a higher return in the new environment. Empirically, it indicates that <strong>episodes receiving higher returns are more in line with the new environment</strong>, i.e., containing more new information.
							</p>
							<p>
								<strong>Re-weighting</strong> <br>
								During parameter updating, we <strong>assign higher importance weights to episodes that contain more new information</strong>, thus encouraging the previous optimum of parameters to be faster adjusted to a new one that fits in the new environment. It may be helpful for the algorithm to escape from those "deceptive" regions adjacent to the parameter space of the previous optimum.
							</p>
							<p align="center">
								$w(\tau^i)=\frac{1}{\rho}(r(\tau^i)+u)$ <br>
								$\nabla_{\theta}J(\theta)=\sum_iw(\tau^i)\nabla_{\theta}\log\pi_{\theta}(\tau^i)r(\tau^i)$
							</p>
							
							
						</td>
					</tr>
				</table>
	
	      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>The integrated algorithm</heading>
            </td>
          </tr>
					<tr>
            <td width="100%" align="center">
							<img src='data/IRL_CS/algorithm.png' width="500"></td>
					</tr>
				</table>
	
				<hr>
		    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Experiments</heading>
              <p>2D navigation tasks, and MuJoCo locomotion tasks.</p>
            </td>
					</tr>
					
					<tr>
						<td width="100%" valign="middle">
							<heading>2D navigation</heading>
							<p>
								A point agent must move to a goal position in 2D. The observation is the current 2D position, and actions correspond to 2D velocity commands clipped to be in the range [-0.1,0.1]. Episodes terminate when the agent is within 0.01 of the goal or at the horizon of H=100.
							</p>
						</td>
					</tr>
					
					<tr>
            <td width="100%" align="center">
							<img src='data/IRL_CS/navi.png' width="500"></td>
					</tr>
					
					<tr>
						<td width="100%" valign="middle">
							<heading>MuJoCo locomotiton</heading>
							<p>
								<strong>Reacher:</strong> This domain consists of moving a two-joint torque-controlled simulated robotic arm to a specific target location.
							</p>
						</td>
					</tr>
					
					<tr>
            <td width="100%" align="center">
							<img src='data/IRL_CS/reacher.gif' width="400"></td>
					</tr>
					
					<tr>
						<td width="100%" valign="middle">
							<table width="100%" align="center" border="0" cellspacing="0" cellpadding="5">
									<tr>
										<td width="30%" align="right">Type I:</td>
										<td width="70%" align="left">
											Varying the target location within the reachable circle randomly. <br>
											The environment changes in the reward functions in this case.
										</td>
									</tr>
									<tr>
										<td width="30%" align="right">Type II:</td>
										<td width="70%" align="left">
											Varying the physical variables of "link0" and "joint0" at random. <br>
											The state transition functions change.
										</td>
									</tr>
									<tr>
										<td width="30%" align="right">Type III:</td>
										<td width="70%" align="left">Varying both the target location and the physical variables randomly. <br>The environment changes in both the reward and state transition functions.</td>
									</tr>
								</table>
						</td>
					</tr>
					
					<tr>
						<td width="100%" valign="middle">
							<p>
								<strong>Swimmer/Hopper/HalfCheetah:</strong> It requires a 2D Swimmer/one-legged Hopper/planar Cheetah robot to swim/hop/run forward at a particular velocity. The dynamic environment is created by changing the goal velocity between 0 and 2 at random.
							</p>
						</td>
					</tr>
					<tr>
				</table>

				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
            <td width="30%">
							<img src='data/IRL_CS/swimmer.gif' width="200">
						</td>
						<td width="30%">
							<img src='data/IRL_CS/hopper.gif' width="200">
						</td>
						<td width="30%">
							<img src='data/IRL_CS/cheetah.gif' width="200">
						</td>
					</tr>
				</table>
					
      </td>
    </tr>
  </table>
</body>

</html>
