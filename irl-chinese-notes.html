<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #07889b;
      text-decoration: none;
    }
    
    a:focus, a:hover {
      color: #e37222;
      text-decoration: none;
    }
    
    body, td, th, tr, p, a {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 16px
    }
		
		p2 {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
    }
		
    strong {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 16px;
    }
    
    heading {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 24px;
			color: #e37222;
    }
    
		heading2 {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; 
    font-size: 20px;
    }
		
    papertitle {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 16px;
      font-weight: 700
    }
    
    name {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size: 40px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
	
	<style> 
		p{line-height:22px} 
		.divcss5-b p{margin:30px auto} 
	</style> 
	
  <link rel="icon" type="image/png" href="data/nju_logo_circle.png">
  <title>Zhi WANG</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
	
	<script type="text/x-mathjax-config">
  	MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	</script>
	<script type="text/javascript"
		src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
</head>

<body>
	<script type="text/x-mathjax-config">
  	MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	</script>
	<script type="text/javascript"
		src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
	
  <table width="61.8%" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p align="center">
                <strong><heading>论文笔记 - 增量式强化学习</heading></strong>
                <p>
                  <a href="index#irl"><papertitle>Incremental reinforcement learning with prioritized sweeping for dynamic environments</papertitle></a>,
									<br>
                                    <a href="index.html"><strong>Zhi Wang</strong></a>, Chunlin Chen, Han-Xiong Li, Daoyi Dong, and Tzyh-Jong Tarn, 
									<br>
									<em>IEEE/ASME Transactions on Mechatronics</em>, 2019. 
									<br><br>
									点击<a href="irl-notes.html">这里</a>获取英文版的论文笔记。
                </p>
              </p>
            </td>
          </tr>
        </table>

				<hr>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>动态环境</heading>
              <p> 传统的强化学习算法通常在静态环境下执行。然而，在很多实际应用中，智能体所面对的环境会是动态的。智能体的状态-动作空间、回报值函数和状态转移函数可能会随着时间而变化。 </p>
            </td>
          </tr>
				</table>
					
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td width="100%" valign="middle">
              <heading>简单示例</heading>
            </td>
					</tr>
				</table>
					
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
            <td width="40%"><img src='data/IRL/IRLcon.jpg' width="300"></td>
            <td width="60%" valign="top">
              <p>
								在时间段$t$，该智能体已经学习到了一个最优策略$\pi^*$。然后，在一个新的时间段$t'$，该马尔科夫决策过程的回报值函数发生了变化。该智能体需要重新调整它的行为策略，找到一个新的最优策略$\pi_n^*$，以适应当前的新环境。
								<br><br>
								<strong>增量学习的目标:</strong> 把新的信息融合到已有的知识系统中; 在环境发生变化时候，把之前时间段学习到的最优策略调整为能适应新环境的最优策略。
								<br><br>
								<strong>问题描述:</strong> 在本文中，我们针对的是回报值函数随着时间会发生改变的动态环境。
              </p>
            </td>
					</tr>
        </table>
	
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td width="100%" valign="middle">
              <heading>增量式学习 对比 迁移学习</heading>
							<p><strong>迁移学习</strong>: 通常需要一组比较多数量的源任务，才可以为目标任务提供一个比较有效的知识基础。</p>
							<p><strong>增量式学习</strong>: 用来处理训练样本随着时间推移而不断获取或学习环境不断变化的学习任务。
							</p>
            </td>
					</tr>
				</table>
	
				<hr>
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td width="100%" valign="middle">
              <strong><heading>提出的方法: 增量式强化学习</heading></strong>
            </td>
					</tr>
				</table>
	
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td width="100%" valign="middle">
              <heading><em>第一步：漂移环境的检测</em></heading>
							<p>
								<strong>漂移环境</strong>会生成新数据并为智能体提供新信息。强化学习从与环境的交互中学习信息，即利用在状态$s$下执行动作$a$所获得的回报值$r$。那么，强化学习算法中的环境漂移指的是某个状态-动作对$(s, a)$的回报值$r$随着环境而发生的变化。
								<br><br>
								<strong>漂移环境被定义为所有在旧环境和新环境中回报值发生变化的状态动作对的集合。</strong>
								<br><br>
								为了获得漂移环境，我们需要观察在旧环境（已知）和新环境（未知）中的回报值函数，才能得到发生变化的部分。所以，我们生成一个<strong>探测器智能体</strong>去执行一个虚拟的学习过程，去随机地探索新环境。对于新环境中给定的状态-动作对，检测器只需要遍历一次。与标准的强化学习过程相比，这个虚拟的学习过程的计算复杂度可以忽略不计。
							</p>
            </td>
					</tr>
					
					<tr>
						<td width="100%" valign="middle">
              <heading><em>第二步：优先迭代漂移环境</em></heading>
							<p>
								<strong>基本设置:</strong> 虽然我们暂时无法构建新环境的完整模型，但我们可以使用旧环境的预测模型来同时训练新环境中的行为策略。
								<br><br>
								然而，由于环境漂移，智能体倾向于执行先前的最优策略并且可能被困住，这将使得它难以找到真正的最优策略。换句话说，由漂移环境产生的新信息往往与已有的知识体系（旧环境的最优策略）相冲突。 
								<br><br>
								<strong>需要一种新的机制来提前削弱这种冲突。</strong>
								<br><br>
								由于漂移环境是新信息的来源，我们可以优先对这部分新信息相关的状态-动态空间的值函数用动态规划算法进行值迭代，迭代时候使用旧环境中的值函数和在新环境中检测到的新的回报值函数。
								<br><br>
								<strong>通过这种方式，增量式强化学习将新信息（新的回报值函数）融入到现有的知识系统中（之前的最优值函数），并削弱了它们之间的冲突。</strong>
							</p>
            </td>
					</tr>
					
					<tr>
						<td width="100%" valign="middle">
              <heading><em>第3步：重新开始学习过程</em></heading>
							<p>
								在针对漂移环境的检测和优先值迭代的基础之上，智能体以一种增量式方式重新执行一个标准学习过程以计算出最终的新的最优策略。
							</p>
            </td>
					</tr>
				</table>
	
	      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>集成算法</heading>
              <p> Env. 表示 environment </p>
            </td>
          </tr>
					<tr>
            <td width="100%" align="center">
							<img src='data/IRL/IRLfc.png' width="500"></td>
					</tr>
				</table>
	
				<hr>
		    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>实验</heading>
              <p>一个简单的迷宫案例，一个基准迷宫案例和一个智能仓库系统。</p>
            </td>
					</tr>
					
					<tr>
						<td width="100%" valign="middle">
							<heading>简单的迷宫案例</heading>
						</td>
					</tr>
				</table>
	
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
            <td width="50%">
							<p align="center">旧环境</p>
							<div align="center"><img src='data/IRL/sMaze.png' width="300"></div>
						</td>
            <td width="50%" valign="top">
							<p align="center">新环境</p>
							<div align="center"><img src='data/IRL/sMazeN.png' width="300"></div>
            </td>
					</tr>
				</table>
					
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td width="100%" valign="middle">
							<heading>基准迷宫案例</heading>
						</td>
					</tr>
					
					<tr>
            <td width="100%" align="center">
							<img src='data/IRL/cMaze.png' width="500">
						</td>
					</tr>
				</table>
					
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
						<td width="100%" valign="middle">
							<heading>智能仓库系统</heading>
						</td>
					</tr>
        </table>
	
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
            <td width="40%"><img src='data/IRL/kiva.png' width="300"></td>
            <td width="60%" valign="top">
              <p>
              	<strong>近年来，大型Kiva机器人系统已成为亚马逊仓库中商业自动运输的一项很有前途的应用。</strong>
								<br><br>
								每个存储扩展架包含多个库存窗格，每个窗格包含多个资源。按顺序，机器人沿着已预定好的从起点到目标点的路径携带一个吊舱，将其交付到订单中指定的特定服务站，最后将吊舱返回。
								<br><br>
								<strong>在实际应用中，存储架和拣选站可能会随时间而变化。此外，在多智能体强化学习问题中，给定的机器人的环境可能由于其他机器人的运动而改变。</strong>
              </p>
            </td>
					</tr>
        </table>
	
				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
					<tr>
            <td width="100%">
							<p align="center">旧环境</p>
							<div align="center"><img src='data/IRL/wh.png' width="600"></div>
						</td>
					</tr>
					<tr>
            <td width="100%" valign="top">
							<p align="center">新环境</p>
							<div align="center"><img src='data/IRL/whN.png' width="600"></div>
            </td>
					</tr>
				</table>
	




      </td>
    </tr>
  </table>
</body>

</html>
